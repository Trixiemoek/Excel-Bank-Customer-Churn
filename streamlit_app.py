# -*- coding: utf-8 -*-
"""streamlit_app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HezOaSkEhs5kepRz57UJHmBGYrRuJbfg
"""
import subprocess

# Check if necessary libraries are installed, if not, install them
try:
    import joblib
except ImportError:
    subprocess.check_call(['pip', 'install', 'joblib'])

try:
    import streamlit as st
except ImportError:
    subprocess.check_call(['pip', 'install', 'streamlit'])

try:
    import pandas as pd
except ImportError:
    subprocess.check_call(['pip', 'install', 'pandas'])

try:
    from sklearn.preprocessing import LabelEncoder, OneHotEncoder
except ImportError:
    subprocess.check_call(['pip', 'install', 'scikit-learn'])

import joblib

# Load the trained model
model = joblib.load('best_model.pkl')

# Instantiate the encoders
label_encoder = LabelEncoder()
ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')

# Predefined column names after OneHotEncoding 'Geography'
geography_columns = ['France', 'Germany', 'Spain']

# Placeholder function for preprocessing
def preprocess_data(data):
    # Perform label encoding on 'Gender' column
    if 'Gender' in data.columns:
        data['Gender'] = label_encoder.transform(data['Gender'].astype(str))
    
    # Perform one-hot encoding on 'Geography' column
    if 'Geography' in data.columns:
        # Fit and transform on 'Geography' column
        geography_encoded = pd.DataFrame(ohe.transform(data[['Geography']]), 
                                         index=data.index, 
                                         columns=geography_columns)
        # Combine encoded 'Geography' with the rest of the data and drop the old 'Geography' column
        data = pd.concat([data.drop('Geography', axis=1), geography_encoded], axis=1)
    
    return data

# Define the Streamlit app
def main():
    st.title('Customer Churn Prediction using Machine Learning')

    # File upload section
    st.sidebar.header('Upload CSV File')
    uploaded_file = st.sidebar.file_uploader("Choose a CSV file", type="csv")

    if uploaded_file is not None:
        # Read the uploaded CSV file
        data = pd.read_csv(uploaded_file)

        # Display the uploaded data
        st.subheader('Uploaded Data:')
        st.write(data)

        # Fit the encoders on the initial data (make sure the data used here is consistent with training)
        label_encoder.fit(['Male', 'Female'])
        ohe.fit(pd.DataFrame(['France', 'Germany', 'Spain'], columns=['Geography']))

        # Preprocess the data
        data = preprocess_data(data)

        # Make predictions
        predictions = model.predict(data)

        # Display predictions
        st.subheader('Predictions:')
        st.write(predictions)

        # If you have the true labels in the data, you can evaluate the model
        # Assuming the true labels column is 'label'
        if 'label' in data.columns:
            y_true = data['label']
            y_pred = model.predict(data.drop(columns=['label']))

            # Calculate evaluation metrics
            accuracy = accuracy_score(y_true, y_pred)
            recall = recall_score(y_true, y_pred, average='binary')  # Adjust average as needed
            precision = precision_score(y_true, y_pred, average='binary')  # Adjust average as needed
            f1 = f1_score(y_true, y_pred, average='binary')  # Adjust average as needed

            # Print evaluation metrics
            st.subheader('Evaluation Metrics:')
            st.write("Accuracy:", accuracy)
            st.write("Recall:", recall)
            st.write("Precision:", precision)
            st.write("F1 Score:", f1)

            # Classification report
            st.subheader("Classification Report:")
            st.write(classification_report(y_true, y_pred))

        # Get feature importances from the model
        if hasattr(model, 'feature_importances_'):
            feature_importance = model.feature_importances_
            # Assuming your features are stored in data
            feature_names = data.columns.tolist()
            # Create a DataFrame to associate feature names with their importances
            feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})
            # Sort the DataFrame by importance in descending order
            feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)
            # Display feature importances
            st.subheader('Feature Importances:')
            st.write(feature_importance_df)

if __name__ == '__main__':
    main()
